{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookair/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import zss\n",
    "from datasets import load_dataset\n",
    "from nltk import edit_distance\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from zss import Node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(write_path: Union[str, bytes, os.PathLike], save_obj: Any):\n",
    "    with open(write_path, \"w\") as f:\n",
    "        json.dump(save_obj, f)\n",
    "        \n",
    "        \n",
    "data = {\n",
    "    'name': 'John', \n",
    "    'age': 30, \n",
    "    'city': 'New York'\n",
    "    }\n",
    "\n",
    "\n",
    "save_json('file.json', data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to load json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John', 'age': 30, 'city': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "def load_json(json_path: Union[str, bytes, os.PathLike]):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "data = load_json('file.json')\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donut Dataset Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple example of a class hierarchy in Python that uses super():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species: Dog\n",
      "Species: Canine\n",
      "Breed: Golden Retriever\n",
      "Species: Canine\n",
      "Breed: Labrador Retriever\n",
      "Color: Yellow\n"
     ]
    }
   ],
   "source": [
    "class Animal:\n",
    "    def __init__(self, species):\n",
    "        self.species = species\n",
    "    \n",
    "    def show_species(self):\n",
    "        print(\"Species:\", self.species)\n",
    "        \n",
    "class Dog(Animal):\n",
    "    def __init__(self, species, breed):\n",
    "            super().__init__(species)\n",
    "            self.breed = breed\n",
    "            \n",
    "    def show_breed(self):\n",
    "        print(\"Breed:\", self.breed)\n",
    "        \n",
    "\n",
    "class Labrador(Dog):\n",
    "    def __init__(self, species, breed, color):\n",
    "        super().__init__(species, breed)\n",
    "        self.color = color\n",
    "\n",
    "    def show_color(self):\n",
    "        print(\"Color:\", self.color)\n",
    "        \n",
    "        \n",
    "# Create instances\n",
    "animal = Animal(\"Dog\")\n",
    "animal.show_species()\n",
    "\n",
    "dog = Dog(\"Canine\", \"Golden Retriever\")\n",
    "dog.show_species()\n",
    "dog.show_breed()\n",
    "\n",
    "labrador = Labrador(\"Canine\", \"Labrador Retriever\", \"Yellow\")\n",
    "labrador.show_species()\n",
    "labrador.show_breed()\n",
    "labrador.show_color()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
    "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string)\n",
    "\n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "    \"\"\"\n",
    "    \n",
    "    def __int__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        donut_model: PreTrainedModel,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        promt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.donut_model = donut_model\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        \n",
    "        self.gt_token_sequences = []\n",
    "        \n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth: # when multiple ground truth parses are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_json = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "            \n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    task_start_token\n",
    "                    + self.donut_model.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + self.donut_model.decoder.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons # load json form list of json\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.donut_model.decoder.add_special_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = self.donut_model.decoder.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels.\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "        \n",
    "        # input_tensor\n",
    "        input_tensor = self.donut_model.encoder.prepare_input(sample[\"image\"], random_padding=self.split == \"train\")\n",
    "        \n",
    "        # input_ids\n",
    "        processed_parse = random.choice(self.gt_token_sequences[idx])   # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = self.donut_model.decoder.tokenizer(\n",
    "            processed_parse,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].spueeze(0)\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            labels = input_ids.clone()\n",
    "            labels[\n",
    "                labels == self.donut_model.decoder.tokenizer.pad_token_id\n",
    "            ] = self.ignore_id  # model doesn't need to predict pad token\n",
    "            labels[\n",
    "                : torch.nonzero(labels == self.prompt_end_token_id).sum() + 1\n",
    "            ] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "            return input_tensor, input_ids, labels\n",
    "        else:\n",
    "            prompt_end_index = torch.nonzero(\n",
    "                input_ids == self.prompt_end_token_id\n",
    "            ).sum()  # return prompt end index instead of target output labels\n",
    "            return input_tensor, input_ids, prompt_end_index, processed_parse\n",
    "\n",
    "\n",
    "class JSONParseEvaluator:\n",
    "    \"\"\"\n",
    "    Calculate n-TED(Normalized Tree Edit Distance) based accuracy and F1 accuracy score\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def flatten(data: dict):\n",
    "        \"\"\"\n",
    "        Convert Dictionary into Non-nested Dictionary\n",
    "        Example:\n",
    "            input(dict)\n",
    "                {\n",
    "                    \"menu\": [\n",
    "                        {\"name\" : [\"cake\"], \"count\" : [\"2\"]},\n",
    "                        {\"name\" : [\"juice\"], \"count\" : [\"1\"]},\n",
    "                    ]\n",
    "                }\n",
    "            output(list)\n",
    "                [\n",
    "                    (\"menu.name\", \"cake\"),\n",
    "                    (\"menu.count\", \"2\"),\n",
    "                    (\"menu.name\", \"juice\"),\n",
    "                    (\"menu.count\", \"1\"),\n",
    "                ]\n",
    "        \"\"\"\n",
    "        flatten_data = list()\n",
    "\n",
    "        def _flatten(value, key=\"\"):\n",
    "            if type(value) is dict:\n",
    "                for child_key, child_value in value.items():\n",
    "                    _flatten(child_value, f\"{key}.{child_key}\" if key else child_key)\n",
    "            elif type(value) is list:\n",
    "                for value_item in value:\n",
    "                    _flatten(value_item, key)\n",
    "            else:\n",
    "                flatten_data.append((key, value))\n",
    "\n",
    "        _flatten(data)\n",
    "        return flatten_data\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def update_cost(node1: Node, node2: Node):\n",
    "        \"\"\"\n",
    "        Update cost for tree edit distance.\n",
    "        If both are leaf node, calculate string edit distance between two labels (special token '<leaf>' will be ignored).\n",
    "        If one of them is leaf node, cost is length of string in leaf node + 1.\n",
    "        If neither are leaf node, cost is 0 if label1 is same with label2 othewise 1\n",
    "        \"\"\"\n",
    "        label1 = node1.label\n",
    "        label2 = node2.label\n",
    "        label1_leaf = \"<leaf>\" in label1\n",
    "        label2_leaf = \"<leaf>\" in label2\n",
    "        if label1_leaf == True and label2_leaf == True:\n",
    "            return edit_distance(label1.replace(\"<leaf>\", \"\"), label2.replace(\"<leaf>\", \"\"))\n",
    "        elif label1_leaf == False and label2_leaf == True:\n",
    "            return 1 + len(label2.replace(\"<leaf>\", \"\"))\n",
    "        elif label1_leaf == True and label2_leaf == False:\n",
    "            return 1 + len(label1.replace(\"<leaf>\", \"\"))\n",
    "        else:\n",
    "            return int(label1 != label2)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def insert_and_remove_cost(node: Node):\n",
    "        \"\"\"\n",
    "        Insert and remove cost for tree edit distance.\n",
    "        If leaf node, cost is length of label name.\n",
    "        Otherwise, 1\n",
    "        \"\"\"\n",
    "        label = node.label\n",
    "        if \"<leaf>\" in label:\n",
    "            return len(label.replace(\"<leaf>\", \"\"))\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "        \n",
    "    def normalize_dict(self, data: Union[Dict, List, Any]):\n",
    "        \"\"\"\n",
    "        Sort by value, while iterate over element if data is list\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return {}\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            new_data = dict()\n",
    "            for key in sorted(data.keys(), key=lambda k: (len(k), k)):\n",
    "                value = self.normalize_dict(data[key])\n",
    "                if value:\n",
    "                    if not isinstance(value, list):\n",
    "                        value = [value]\n",
    "                    new_data[key] = value\n",
    "\n",
    "        elif isinstance(data, list):\n",
    "            if all(isinstance(item, dict) for item in data):\n",
    "                new_data = []\n",
    "                for item in data:\n",
    "                    item = self.normalize_dict(item)\n",
    "                    if item:\n",
    "                        new_data.append(item)\n",
    "            else:\n",
    "                new_data = [str(item).strip() for item in data if type(item) in {str, int, float} and str(item).strip()]\n",
    "        else:\n",
    "            new_data = [str(data).strip()]\n",
    "\n",
    "        return new_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    def cal_f1(self, preds: List[dict], answers: List[dict]):\n",
    "        \"\"\"\n",
    "        Calculate global F1 accuracy score (field-level, micro-averaged) by counting all true positives, false negatives and false positives\n",
    "        \"\"\"\n",
    "        total_tp, total_fn_or_fp = 0, 0\n",
    "        for pred, answer in zip(preds, answers):\n",
    "            pred, answer = self.flatten(self.normalize_dict(pred)), self.flatten(self.normalize_dict(answer))\n",
    "            for field in pred:\n",
    "                if field in answer:\n",
    "                    total_tp += 1\n",
    "                    answer.remove(field)\n",
    "                else:\n",
    "                    total_fn_or_fp += 1\n",
    "            total_fn_or_fp += len(answer)\n",
    "        return total_tp / (total_tp + total_fn_or_fp / 2)\n",
    "    \n",
    "    \n",
    "    def construct_tree_from_dict(self, data: Union[Dict, List], node_name: str = None):\n",
    "        \"\"\"\n",
    "        Convert Dictionary into Tree\n",
    "\n",
    "        Example:\n",
    "            input(dict)\n",
    "\n",
    "                {\n",
    "                    \"menu\": [\n",
    "                        {\"name\" : [\"cake\"], \"count\" : [\"2\"]},\n",
    "                        {\"name\" : [\"juice\"], \"count\" : [\"1\"]},\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "            output(tree)\n",
    "                                        <root>\n",
    "                                        |\n",
    "                                        menu\n",
    "                                    /    \\\n",
    "                                <subtree>  <subtree>\n",
    "                            /      |     |      \\\n",
    "                            name    count  name    count\n",
    "                        /         |     |         \\\n",
    "                    <leaf>cake  <leaf>2  <leaf>juice  <leaf>1\n",
    "            \"\"\"\n",
    "        if node_name is None:\n",
    "            node_name = \"<root>\"\n",
    "            \n",
    "        node = Node(node_name)\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                kid_node = self.construct_tree_from_dict(value, key)\n",
    "        elif isinstance(data, list):\n",
    "            if all(isinstance(item, dict) for item in data):\n",
    "                for item in data:\n",
    "                    kid_node = self.construct_tree_from_dict(\n",
    "                        item,\n",
    "                        \"<subtree>\",\n",
    "                    )\n",
    "                    node.addkid(kid_node)\n",
    "            else:\n",
    "                for item in data:\n",
    "                    node.addkid(Node(f\"<leaf>{item}\"))\n",
    "        else:\n",
    "            raise Exception(data, node_name)\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    \n",
    "    def cal_acc(self, pred: dict, answer: dict):\n",
    "        \"\"\"\n",
    "        Calculate normalized tree edit distance(nTED) based accuracy.\n",
    "        1) Construct tree from dict,\n",
    "        2) Get tree distance with insert/remove/update cost,\n",
    "        3) Divide distance with GT tree size (i.e., nTED),\n",
    "        4) Calculate nTED based accuracy. (= max(1 - nTED, 0 ).\n",
    "        \"\"\"\n",
    "        pred = self.construct_tree_from_dict(self.normalize_dict(pred))\n",
    "        answer = self.construct_tree_from_dict(self.normalize_dict(answer))\n",
    "        return max(\n",
    "            0,\n",
    "            1\n",
    "            - (\n",
    "                zss.distance(\n",
    "                    pred,\n",
    "                    answer,\n",
    "                    get_children=zss.Node.get_children,\n",
    "                    insert_cost=self.insert_and_remove_cost,\n",
    "                    remove_cost=self.insert_and_remove_cost,\n",
    "                    update_cost=self.update_cost,\n",
    "                    return_operations=False,\n",
    "                )\n",
    "                / zss.distance(\n",
    "                    self.construct_tree_from_dict(self.normalize_dict({})),\n",
    "                    answer,\n",
    "                    get_children=zss.Node.get_children,\n",
    "                    insert_cost=self.insert_and_remove_cost,\n",
    "                    remove_cost=self.insert_and_remove_cost,\n",
    "                    update_cost=self.update_cost,\n",
    "                    return_operations=False,\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
